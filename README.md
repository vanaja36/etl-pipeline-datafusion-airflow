# ETL Pipeline with Data Fusion, Airflow, and BigQuery

## üìå About
This project demonstrates an **end-to-end ETL (Extract, Transform, Load) pipeline** built on **Google Cloud Platform** using:
- **Python** for data extraction and CSV generation
- **Google Cloud Storage (GCS)** as staging
- **Google Cloud Data Fusion** for transformation
- **BigQuery** as the data warehouse
- **Apache Airflow** for orchestration

The pipeline automates the process of generating synthetic employee data, staging it in GCS, transforming it in Data Fusion, and loading the cleaned data into BigQuery for analytics.


## üõ†Ô∏è Tools & Technologies
- **Google Cloud Data Fusion** ‚Üí Data ingestion & transformation  
- **Apache Airflow** ‚Üí Workflow orchestration & scheduling  
- **Google BigQuery** ‚Üí Data warehouse for analytics  
- **Google Cloud Storage (GCS)** ‚Üí Staging area for raw/processed data  
- **Python (Faker, GCS SDK)** ‚Üí Data generation & extraction  
- **SQL** ‚Üí Data validation  

## üîÑ Workflow Steps
1. **Extract (extract.py)**  
   - Generates synthetic employee data (100+ rows) using `Faker`.  
   - Saves data into a timestamped CSV file.  
   - Uploads CSV into GCS bucket (`gs://bkt-employee-data/employee_data/`).  

2. **Transform (Data Fusion Pipeline)**  
   - Cleans employee data.  
   - Removes duplicates, standardizes formats, and maps schema.  

3. **Load (BigQuery)**  
   - Transformed data is written into BigQuery table:  
     `project.dataset.employee_data`  

4. **Orchestration (Airflow DAG)**  
   - Runs `extract.py` to generate/upload CSV.  
   - Triggers Data Fusion pipeline.  
   - (Optional) Runs BigQuery validation SQL to confirm data load.  


